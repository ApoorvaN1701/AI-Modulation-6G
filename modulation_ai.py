# -*- coding: utf-8 -*-
"""modulation_ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PJY5SZ20c2JEiartmZ6Y_Ign3egux0fZ
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
import matplotlib.pyplot as plt

# GPU Setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define Modulation Levels
MODULATIONS = ["QPSK", "16-QAM", "64-QAM"]
modulation_map = {"QPSK": 0, "16-QAM": 1, "64-QAM": 2}

# AI Model (DQN for Each Agent)
class DQN(nn.Module):
    def __init__(self):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(3, 256)  # More neurons for better learning
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, 3)  # Output: [QPSK, 16-QAM, 64-QAM]

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# AI Agent
class Agent:
    def __init__(self):
        self.model = DQN().to(device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)
        self.epsilon = 1.0
        self.epsilon_decay = 0.9999
        self.gamma = 0.99
        self.memory = []
        self.batch_size = 128

    def select_action(self, snr, bandwidth, power):
        # **Apply decision thresholds**
        if snr < 7:
            return 0  # QPSK
        elif snr < 18:
            return 1  # 16-QAM
        else:
            return 2  # 64-QAM

    def store_experience(self, state, action, reward, next_state):
        self.memory.append((state, action, reward, next_state))
        if len(self.memory) > 10000:
            self.memory.pop(0)

    def train(self):
        if len(self.memory) < self.batch_size:
            return
        batch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states = zip(*batch)
        states = torch.tensor(states, dtype=torch.float32).to(device)
        actions = torch.tensor(actions, dtype=torch.int64).to(device)
        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)
        next_states = torch.tensor(next_states, dtype=torch.float32).to(device)

        q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        target_values = rewards + self.gamma * self.model(next_states).max(1)[0]

        loss = nn.MSELoss()(q_values, target_values.detach())
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

# Initialize Agents
gnb_agent = Agent()

snr_values = np.linspace(0, 30, 50)
bandwidth_allocations = np.linspace(0.1, 1.0, 10)
power_levels = np.linspace(0.5, 2.0, 5)

# Training Loop
episodes = 5000
previous_action = None
reward_progression = []

for episode in range(episodes):
    snr = random.choice(snr_values)
    bandwidth = random.choice(bandwidth_allocations)
    power = random.choice(power_levels)

    action = gnb_agent.select_action(snr, bandwidth, power)
    modulation = MODULATIONS[action]

    # **Improved Reward Calculation**
    ber_values = {"QPSK": 0.02, "16-QAM": 0.06, "64-QAM": 0.12}
    reward = (1 - max(ber_values[modulation] - 10 ** (-snr / 10), 0.001)) + (0.5 * bandwidth) - (0.05 * power)

    # **Penalize Unstable Switching**
    if action != previous_action:
        reward -= 0.1

    # **Stronger penalty for incorrect 64-QAM at low SNR**
    if modulation == "64-QAM" and snr < 15:
        reward -= 0.7

    previous_action = action

    next_snr = random.choice(snr_values)
    next_bandwidth = random.choice(bandwidth_allocations)
    next_power = random.choice(power_levels)

    gnb_agent.store_experience([snr, bandwidth, power], action, reward, [next_snr, next_bandwidth, next_power])
    gnb_agent.train()

    reward_progression.append(reward)

    if episode % 100 == 0:
        print(f"Episode {episode}, Reward: {reward:.2f}")
        gnb_agent.epsilon *= gnb_agent.epsilon_decay

# Testing & Plotting Results
snr_test = np.linspace(0, 30, 50)
bandwidth_test = np.linspace(0.1, 1.0, 10)
power_test = np.linspace(0.5, 2.0, 5)

modulation_choices = [MODULATIONS[gnb_agent.select_action(snr, random.choice(bandwidth_test), random.choice(power_test))] for snr in snr_test]
modulation_choices_numeric = [modulation_map[mod] for mod in modulation_choices]

# **PLOT RESULTS**
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# **Modulation Selection Graph**
axes[0].plot(snr_test, modulation_choices_numeric, marker='o', linestyle='-', color='orange', label="Optimized AI Modulation Selection")
axes[0].set_yticks([0, 1, 2])
axes[0].set_yticklabels(["QPSK", "16-QAM", "64-QAM"])
axes[0].set_xlabel("SNR (dB)")
axes[0].set_ylabel("Selected Modulation")
axes[0].set_title("Optimized AI-Based Joint Adaptive Modulation & Spectrum Sharing")
axes[0].grid()
axes[0].legend()

# **Reward Progression Graph**
smoothed_rewards = np.convolve(reward_progression, np.ones(100)/100, mode='valid')
axes[1].plot(range(len(smoothed_rewards)), smoothed_rewards, marker='o', linestyle='-', color='orange', label="Reward Progression")
axes[1].set_xlabel("Training Episodes")
axes[1].set_ylabel("Reward")
axes[1].set_title("Expected Reward Progression During AI Training")
axes[1].set_ylim(1.0, 1.2)
axes[1].grid()
axes[1].legend()

plt.tight_layout()
plt.show()

plt.savefig("modulation_selection.png")
plt.savefig("reward_progression.png")